{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12ec9ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import skimage.measure\n",
    "import pandas as pd\n",
    "from scipy.spatial import distance\n",
    "import skimage.measure\n",
    "from skimage.morphology import remove_small_objects,closing,binary_closing\n",
    "from scipy import ndimage\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "from datetime import datetime,timedelta\n",
    "import os\n",
    "from dask.distributed import Client\n",
    "import glob\n",
    "from mrms_gage import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a97a0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c205761b",
   "metadata": {},
   "source": [
    "define paths and filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a45504b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a path to the code file\n",
    "codeDir = os.path.dirname(os.path.abspath(os.getcwd()))\n",
    "parentDir = os.path.dirname(codeDir)\n",
    "\n",
    "precip_folder = os.path.join(parentDir,\"precip_stats\")\n",
    "storm_folder = os.path.join(parentDir,\"CO_storm_output\")\n",
    "mrms_folder = os.path.join(parentDir,\"MRMS\",\"2min_rate_cat_month_CO\",\"eval\")\n",
    "\n",
    "unique = np.arange(0,0.1,0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "809377cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(mrms_folder)\n",
    "filenames_rate = glob.glob('*.grib2')\n",
    "os.chdir(storm_folder)\n",
    "filenames_storm = glob.glob('*.nc')\n",
    "os.chdir(precip_folder) \n",
    "filenames_accum = glob.glob('*_accum')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a14da88",
   "metadata": {},
   "source": [
    "calculate multisensor correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "584578f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring index file 'Z:\\\\MRMS\\\\1hr_QPE_multi_cat_yr_CO\\\\2022_multiQPE_CO.grib2.923a8.idx' incompatible with GRIB file\n"
     ]
    }
   ],
   "source": [
    "# multisensor correction\n",
    "# open multisensor qpe for 2021,2022\n",
    "mrms_multi_folder = os.path.join(parentDir,\"MRMS\",\"1hr_QPE_multi_cat_yr_CO\")\n",
    "filenames_multi = glob.glob(mrms_multi_folder+'\\\\'+'*.grib2')\n",
    "mrms_multi = xr.open_mfdataset(filenames_multi,engine = \"cfgrib\",chunks={'time': '1MB'})\n",
    "\n",
    "# open radaronly qpe for 2021,2022\n",
    "mrms_radar_folder = os.path.join(parentDir,\"MRMS\",\"1hr_QPE_radar_cat_yr_CO\")\n",
    "filenames_radar = glob.glob(mrms_radar_folder+'\\\\'+'*.grib2')\n",
    "mrms_radar = xr.open_mfdataset(filenames_radar,engine = \"cfgrib\",chunks={'time': '1MB'})\n",
    "\n",
    "correction = (mrms_multi/mrms_radar)\n",
    "correction = correction.where(correction.unknown != np.inf).fillna(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b512ccac",
   "metadata": {},
   "source": [
    "open above threshold values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2a366ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "temp_folder = os.path.join(parentDir,\"predict_temp\")\n",
    "os.chdir(temp_folder)\n",
    "filenames_temp = glob.glob('*8hrsampleabove20mm.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "edff6b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "accum_t=[]\n",
    "for i in range(len(filenames_temp)):\n",
    "    a = xr.open_dataset(temp_folder+'\\\\'+filenames_temp[i],chunks={'time': '50MB'})\n",
    "    accum_t.append(a)\n",
    "\n",
    "accum_t = xr.concat(accum_t,dim='time')\n",
    "accum_t = accum_t.sortby('time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d62720",
   "metadata": {},
   "outputs": [],
   "source": [
    "accum_t = accum_t.to_dataframe().drop(columns=['step','heightAboveSea']).dropna().reset_index()\n",
    "predict = accum_t.rename(columns={\"unknown\": \"totalaccum_point\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cba39d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# month\n",
    "predict['month']=[predict['time'][i].month for i in range(len(predict))]\n",
    "# hour\n",
    "predict['hour']=[predict['time'][i].hour for i in range(len(predict))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715ce0e6",
   "metadata": {},
   "source": [
    "open rate and storm_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06dabbed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Can't read index file 'aug_2021_rate_CO.grib2.923a8.idx'\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\whitep\\.conda\\envs\\radar\\lib\\site-packages\\cfgrib\\messages.py\", line 547, in from_indexpath_or_filestream\n",
      "    self = cls.from_indexpath(indexpath)\n",
      "  File \"C:\\Users\\whitep\\.conda\\envs\\radar\\lib\\site-packages\\cfgrib\\messages.py\", line 429, in from_indexpath\n",
      "    index = pickle.load(file)\n",
      "EOFError: Ran out of input\n",
      "Ignoring index file 'aug_2022_rate_CO.grib2.923a8.idx' incompatible with GRIB file\n",
      "Ignoring index file 'jul_2022_rate_CO.grib2.923a8.idx' incompatible with GRIB file\n"
     ]
    }
   ],
   "source": [
    "os.chdir(mrms_folder)\n",
    "mrms_rate = xr.open_mfdataset(filenames_rate,engine = \"cfgrib\",chunks={'time': '50MB'})\n",
    "mrms_rate = mrms_rate.where(mrms_rate.longitude<=256,drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "696960ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'predict_temp.csv'\n",
    "output = parentDir+name\n",
    "predict.to_csv(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5051a52",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\whitep\\.conda\\envs\\radar\\lib\\site-packages\\xarray\\core\\indexing.py:1374: PerformanceWarning: Slicing is producing a large chunk. To accept the large\n",
      "chunk and silence this warning, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': False}):\n",
      "    ...     array[indexer]\n",
      "\n",
      "To avoid creating the large chunks, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': True}):\n",
      "    ...     array[indexer]\n",
      "  return self.array[key]\n",
      "C:\\Users\\whitep\\.conda\\envs\\radar\\lib\\site-packages\\xarray\\core\\indexing.py:1380: PerformanceWarning: Slicing is producing a large chunk. To accept the large\n",
      "chunk and silence this warning, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': False}):\n",
      "    ...     array[indexer]\n",
      "\n",
      "To avoid creating the large chunks, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': True}):\n",
      "    ...     array[indexer]\n",
      "  value = value[(slice(None),) * axis + (subkey,)]\n",
      "C:\\Users\\whitep\\.conda\\envs\\radar\\lib\\site-packages\\xarray\\core\\indexing.py:1374: PerformanceWarning: Slicing is producing a large chunk. To accept the large\n",
      "chunk and silence this warning, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': False}):\n",
      "    ...     array[indexer]\n",
      "\n",
      "To avoid creating the large chunks, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': True}):\n",
      "    ...     array[indexer]\n",
      "  return self.array[key]\n"
     ]
    }
   ],
   "source": [
    "# apply correction and calculate 15min intensity\n",
    "\n",
    "mrms_accum = mrms_rate*(2/60)\n",
    "# multisensor correction\n",
    "correction_month = correction.sel(time=slice(mrms_accum.time[0],mrms_accum.time[-1]))\n",
    "correction_month = correction_month.resample(time='2min').pad()\n",
    "mrms_2_corrected = mrms_accum*correction_month\n",
    "# change time to MST\n",
    "mrms_2_corrected = time_change(mrms_2_corrected)\n",
    "# get rid of neg values\n",
    "mrms_2_corrected = mrms_2_corrected.where(mrms_2_corrected>=0)\n",
    "\n",
    "mrms_one = mrms_2_corrected.resample(time='1min').asfreq()\n",
    "mrms_one = mrms_one.unknown.fillna(0)\n",
    "mrms_15 = (mrms_one.rolling(time=15,min_periods=1).sum())*(60/15) \n",
    "\n",
    "mrms_15_21 = mrms_15.sel(time=slice('2021-04-30 18:00','2021-09-30 17:00'))\n",
    "mrms_15_22 = mrms_15.sel(time=slice('2022-04-30 18:00','2022-09-30 17:00'))\n",
    "mrms_15 = xr.concat([mrms_15_21,mrms_15_22],dim='time').persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619a1510",
   "metadata": {},
   "outputs": [],
   "source": [
    "point_mean_accum = mrms_2_corrected.resample(time='8h').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "9944dcc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict=pd.read_csv(parentDir+'\\\\predict_temp.csv',index_col=0)\n",
    "predict.sort_values(by=['time'])\n",
    "predict.time = predict.time.astype('datetime64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "631b07b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(temp_folder)\n",
    "filenames_int = glob.glob('*15int.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "81f75aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_int_point = []\n",
    "max_std_point = []\n",
    "max_var_point = []\n",
    "max_mean_point = []\n",
    "max_median_point = []\n",
    "\n",
    "for i in range(len(filenames_int)):\n",
    "    mrms_rate = xr.open_dataset(filenames_int[i],chunks={'time': '1MB'})\n",
    "    \n",
    "    m = mrms_rate.resample(time='8h').max()\n",
    "    m = m.where(m.time.isin(predict.time),drop=True)\n",
    "    m = m.to_dataframe()\n",
    "    max_int_point.append(m)\n",
    "\n",
    "    m = mrms_rate.resample(time='8h').std()\n",
    "    m = m.where(m.time.isin(predict.time),drop=True)\n",
    "    m = m.to_dataframe()\n",
    "    max_std_point.append(m)\n",
    "    \n",
    "    m = mrms_rate.resample(time='8h').var()\n",
    "    m = m.where(m.time.isin(predict.time),drop=True)\n",
    "    m = m.to_dataframe()\n",
    "    max_var_point.append(m)\n",
    "    \n",
    "    m = mrms_rate.resample(time='8h').mean()\n",
    "    m = m.where(m.time.isin(predict.time),drop=True)\n",
    "    m = m.to_dataframe()\n",
    "    max_mean_point.append(m)\n",
    "    \n",
    "    m = mrms_rate.resample(time='8h').median()\n",
    "    m = m.where(m.time.isin(predict.time),drop=True)\n",
    "    m = m.to_dataframe()\n",
    "    max_median_point.append(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "95224a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict.longitude = predict.longitude.round(decimals=4)\n",
    "predict.latitude = predict.latitude.round(decimals=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f15dc48",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_int_point = pd.concat(max_int_point).reset_index()\n",
    "\n",
    "max_int_point.longitude = max_int_point.longitude.round(decimals=4)\n",
    "max_int_point.latitude = max_int_point.latitude.round(decimals=4)\n",
    "\n",
    "predict = predict.merge(max_int_point,on=['time','latitude','longitude'])\n",
    "\n",
    "predict=predict.dropna()\n",
    "predict=predict.loc[predict.unknown>0]\n",
    "predict=predict.groupby(['time','latitude','longitude']).max().reset_index(drop=True)\n",
    "predict = predict.rename(columns={'unknown':'max_int_point'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "5869c7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_std_point = pd.concat(max_std_point).reset_index()\n",
    "\n",
    "max_std_point.longitude = max_std_point.longitude.round(decimals=4)\n",
    "max_std_point.latitude = max_std_point.latitude.round(decimals=4)\n",
    "\n",
    "predict = predict.merge(max_std_point,on=['time','latitude','longitude'])\n",
    "\n",
    "predict=predict.dropna()\n",
    "predict=predict.loc[predict.unknown>0]\n",
    "predict=predict.groupby(['time','latitude','longitude']).max().reset_index(drop=True)\n",
    "predict = predict.rename(columns={'unknown':'max_std_point'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "54667fc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>totalaccum_point</th>\n",
       "      <th>month</th>\n",
       "      <th>hour</th>\n",
       "      <th>storm_id</th>\n",
       "      <th>accum_mean_storm</th>\n",
       "      <th>accum_max_storm</th>\n",
       "      <th>index</th>\n",
       "      <th>max</th>\n",
       "      <th>std</th>\n",
       "      <th>var</th>\n",
       "      <th>...</th>\n",
       "      <th>eccentricity</th>\n",
       "      <th>velocity_storm</th>\n",
       "      <th>test_x</th>\n",
       "      <th>max_int_point</th>\n",
       "      <th>step_x</th>\n",
       "      <th>heightAboveSea_x</th>\n",
       "      <th>test_y</th>\n",
       "      <th>max_std_point</th>\n",
       "      <th>step_y</th>\n",
       "      <th>heightAboveSea_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>21.529583</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>[[4979.06]]</td>\n",
       "      <td>11.672566</td>\n",
       "      <td>178.783340</td>\n",
       "      <td>223708</td>\n",
       "      <td>69.200005</td>\n",
       "      <td>1.31856</td>\n",
       "      <td>1.440746</td>\n",
       "      <td>...</td>\n",
       "      <td>0.650019</td>\n",
       "      <td>14.674299</td>\n",
       "      <td>2021-05-02 08:00:0038.395255.995</td>\n",
       "      <td>56.285561</td>\n",
       "      <td>0 days</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2021-05-02 08:00:0038.395255.995</td>\n",
       "      <td>9.697265</td>\n",
       "      <td>0 days</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21.398945</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>[[4979.06]]</td>\n",
       "      <td>11.672566</td>\n",
       "      <td>178.783340</td>\n",
       "      <td>224877</td>\n",
       "      <td>69.200005</td>\n",
       "      <td>1.31856</td>\n",
       "      <td>1.440746</td>\n",
       "      <td>...</td>\n",
       "      <td>0.650019</td>\n",
       "      <td>14.674299</td>\n",
       "      <td>2021-05-02 08:00:0038.405255.955</td>\n",
       "      <td>54.272572</td>\n",
       "      <td>0 days</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2021-05-02 08:00:0038.405255.955</td>\n",
       "      <td>9.191720</td>\n",
       "      <td>0 days</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>24.162045</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>[[4979.06]]</td>\n",
       "      <td>11.672566</td>\n",
       "      <td>178.783340</td>\n",
       "      <td>224899</td>\n",
       "      <td>69.200005</td>\n",
       "      <td>1.31856</td>\n",
       "      <td>1.440746</td>\n",
       "      <td>...</td>\n",
       "      <td>0.650019</td>\n",
       "      <td>14.674299</td>\n",
       "      <td>2021-05-02 08:00:0038.405255.985</td>\n",
       "      <td>54.613335</td>\n",
       "      <td>0 days</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2021-05-02 08:00:0038.405255.985</td>\n",
       "      <td>10.419754</td>\n",
       "      <td>0 days</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>22.641127</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>[[4979.06]]</td>\n",
       "      <td>11.672566</td>\n",
       "      <td>178.783340</td>\n",
       "      <td>224907</td>\n",
       "      <td>69.200005</td>\n",
       "      <td>1.31856</td>\n",
       "      <td>1.440746</td>\n",
       "      <td>...</td>\n",
       "      <td>0.650019</td>\n",
       "      <td>14.674299</td>\n",
       "      <td>2021-05-02 08:00:0038.405255.995</td>\n",
       "      <td>50.834641</td>\n",
       "      <td>0 days</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2021-05-02 08:00:0038.405255.995</td>\n",
       "      <td>9.435122</td>\n",
       "      <td>0 days</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.298344</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>[[4979.06]]</td>\n",
       "      <td>11.672566</td>\n",
       "      <td>178.783340</td>\n",
       "      <td>226159</td>\n",
       "      <td>69.200005</td>\n",
       "      <td>1.31856</td>\n",
       "      <td>1.440746</td>\n",
       "      <td>...</td>\n",
       "      <td>0.650019</td>\n",
       "      <td>14.674299</td>\n",
       "      <td>2021-05-02 08:00:0038.415255.985</td>\n",
       "      <td>56.229160</td>\n",
       "      <td>0 days</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2021-05-02 08:00:0038.415255.985</td>\n",
       "      <td>9.170878</td>\n",
       "      <td>0 days</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>468466</th>\n",
       "      <td>21.511593</td>\n",
       "      <td>9</td>\n",
       "      <td>16</td>\n",
       "      <td>[[251126.09]]</td>\n",
       "      <td>3.530297</td>\n",
       "      <td>52.100002</td>\n",
       "      <td>402153</td>\n",
       "      <td>108.773331</td>\n",
       "      <td>1.97308</td>\n",
       "      <td>3.803851</td>\n",
       "      <td>...</td>\n",
       "      <td>0.879052</td>\n",
       "      <td>16.720377</td>\n",
       "      <td>2022-09-30 16:00:0040.025255.955</td>\n",
       "      <td>84.113037</td>\n",
       "      <td>0 days</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2022-09-30 16:00:0040.025255.955</td>\n",
       "      <td>20.756189</td>\n",
       "      <td>0 days</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>468467</th>\n",
       "      <td>24.021246</td>\n",
       "      <td>9</td>\n",
       "      <td>16</td>\n",
       "      <td>[[251126.09]]</td>\n",
       "      <td>3.530297</td>\n",
       "      <td>52.100002</td>\n",
       "      <td>403045</td>\n",
       "      <td>108.773331</td>\n",
       "      <td>1.97308</td>\n",
       "      <td>3.803851</td>\n",
       "      <td>...</td>\n",
       "      <td>0.879052</td>\n",
       "      <td>16.720377</td>\n",
       "      <td>2022-09-30 16:00:0040.035255.945</td>\n",
       "      <td>88.298317</td>\n",
       "      <td>0 days</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2022-09-30 16:00:0040.035255.945</td>\n",
       "      <td>21.920897</td>\n",
       "      <td>0 days</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>468468</th>\n",
       "      <td>23.270588</td>\n",
       "      <td>9</td>\n",
       "      <td>16</td>\n",
       "      <td>[[251126.09]]</td>\n",
       "      <td>3.530297</td>\n",
       "      <td>52.100002</td>\n",
       "      <td>403050</td>\n",
       "      <td>108.773331</td>\n",
       "      <td>1.97308</td>\n",
       "      <td>3.803851</td>\n",
       "      <td>...</td>\n",
       "      <td>0.879052</td>\n",
       "      <td>16.720377</td>\n",
       "      <td>2022-09-30 16:00:0040.035255.955</td>\n",
       "      <td>85.909019</td>\n",
       "      <td>0 days</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2022-09-30 16:00:0040.035255.955</td>\n",
       "      <td>20.881594</td>\n",
       "      <td>0 days</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>468469</th>\n",
       "      <td>24.033580</td>\n",
       "      <td>9</td>\n",
       "      <td>16</td>\n",
       "      <td>[[251126.09]]</td>\n",
       "      <td>3.530297</td>\n",
       "      <td>52.100002</td>\n",
       "      <td>403944</td>\n",
       "      <td>108.773331</td>\n",
       "      <td>1.97308</td>\n",
       "      <td>3.803851</td>\n",
       "      <td>...</td>\n",
       "      <td>0.879052</td>\n",
       "      <td>16.720377</td>\n",
       "      <td>2022-09-30 16:00:0040.045255.945</td>\n",
       "      <td>86.747650</td>\n",
       "      <td>0 days</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2022-09-30 16:00:0040.045255.945</td>\n",
       "      <td>22.126396</td>\n",
       "      <td>0 days</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>468470</th>\n",
       "      <td>22.672130</td>\n",
       "      <td>9</td>\n",
       "      <td>16</td>\n",
       "      <td>[[251126.09]]</td>\n",
       "      <td>3.530297</td>\n",
       "      <td>52.100002</td>\n",
       "      <td>403947</td>\n",
       "      <td>108.773331</td>\n",
       "      <td>1.97308</td>\n",
       "      <td>3.803851</td>\n",
       "      <td>...</td>\n",
       "      <td>0.879052</td>\n",
       "      <td>16.720377</td>\n",
       "      <td>2022-09-30 16:00:0040.045255.955</td>\n",
       "      <td>81.408516</td>\n",
       "      <td>0 days</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2022-09-30 16:00:0040.045255.955</td>\n",
       "      <td>20.442209</td>\n",
       "      <td>0 days</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>468471 rows Ã— 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        totalaccum_point  month  hour       storm_id  accum_mean_storm  \\\n",
       "0              21.529583      5     8    [[4979.06]]         11.672566   \n",
       "1              21.398945      5     8    [[4979.06]]         11.672566   \n",
       "2              24.162045      5     8    [[4979.06]]         11.672566   \n",
       "3              22.641127      5     8    [[4979.06]]         11.672566   \n",
       "4              20.298344      5     8    [[4979.06]]         11.672566   \n",
       "...                  ...    ...   ...            ...               ...   \n",
       "468466         21.511593      9    16  [[251126.09]]          3.530297   \n",
       "468467         24.021246      9    16  [[251126.09]]          3.530297   \n",
       "468468         23.270588      9    16  [[251126.09]]          3.530297   \n",
       "468469         24.033580      9    16  [[251126.09]]          3.530297   \n",
       "468470         22.672130      9    16  [[251126.09]]          3.530297   \n",
       "\n",
       "        accum_max_storm   index         max      std       var  ...  \\\n",
       "0            178.783340  223708   69.200005  1.31856  1.440746  ...   \n",
       "1            178.783340  224877   69.200005  1.31856  1.440746  ...   \n",
       "2            178.783340  224899   69.200005  1.31856  1.440746  ...   \n",
       "3            178.783340  224907   69.200005  1.31856  1.440746  ...   \n",
       "4            178.783340  226159   69.200005  1.31856  1.440746  ...   \n",
       "...                 ...     ...         ...      ...       ...  ...   \n",
       "468466        52.100002  402153  108.773331  1.97308  3.803851  ...   \n",
       "468467        52.100002  403045  108.773331  1.97308  3.803851  ...   \n",
       "468468        52.100002  403050  108.773331  1.97308  3.803851  ...   \n",
       "468469        52.100002  403944  108.773331  1.97308  3.803851  ...   \n",
       "468470        52.100002  403947  108.773331  1.97308  3.803851  ...   \n",
       "\n",
       "        eccentricity  velocity_storm                            test_x  \\\n",
       "0           0.650019       14.674299  2021-05-02 08:00:0038.395255.995   \n",
       "1           0.650019       14.674299  2021-05-02 08:00:0038.405255.955   \n",
       "2           0.650019       14.674299  2021-05-02 08:00:0038.405255.985   \n",
       "3           0.650019       14.674299  2021-05-02 08:00:0038.405255.995   \n",
       "4           0.650019       14.674299  2021-05-02 08:00:0038.415255.985   \n",
       "...              ...             ...                               ...   \n",
       "468466      0.879052       16.720377  2022-09-30 16:00:0040.025255.955   \n",
       "468467      0.879052       16.720377  2022-09-30 16:00:0040.035255.945   \n",
       "468468      0.879052       16.720377  2022-09-30 16:00:0040.035255.955   \n",
       "468469      0.879052       16.720377  2022-09-30 16:00:0040.045255.945   \n",
       "468470      0.879052       16.720377  2022-09-30 16:00:0040.045255.955   \n",
       "\n",
       "        max_int_point  step_x  heightAboveSea_x  \\\n",
       "0           56.285561  0 days               0.0   \n",
       "1           54.272572  0 days               0.0   \n",
       "2           54.613335  0 days               0.0   \n",
       "3           50.834641  0 days               0.0   \n",
       "4           56.229160  0 days               0.0   \n",
       "...               ...     ...               ...   \n",
       "468466      84.113037  0 days               0.0   \n",
       "468467      88.298317  0 days               0.0   \n",
       "468468      85.909019  0 days               0.0   \n",
       "468469      86.747650  0 days               0.0   \n",
       "468470      81.408516  0 days               0.0   \n",
       "\n",
       "                                  test_y  max_std_point  step_y  \\\n",
       "0       2021-05-02 08:00:0038.395255.995       9.697265  0 days   \n",
       "1       2021-05-02 08:00:0038.405255.955       9.191720  0 days   \n",
       "2       2021-05-02 08:00:0038.405255.985      10.419754  0 days   \n",
       "3       2021-05-02 08:00:0038.405255.995       9.435122  0 days   \n",
       "4       2021-05-02 08:00:0038.415255.985       9.170878  0 days   \n",
       "...                                  ...            ...     ...   \n",
       "468466  2022-09-30 16:00:0040.025255.955      20.756189  0 days   \n",
       "468467  2022-09-30 16:00:0040.035255.945      21.920897  0 days   \n",
       "468468  2022-09-30 16:00:0040.035255.955      20.881594  0 days   \n",
       "468469  2022-09-30 16:00:0040.045255.945      22.126396  0 days   \n",
       "468470  2022-09-30 16:00:0040.045255.955      20.442209  0 days   \n",
       "\n",
       "        heightAboveSea_y  \n",
       "0                    0.0  \n",
       "1                    0.0  \n",
       "2                    0.0  \n",
       "3                    0.0  \n",
       "4                    0.0  \n",
       "...                  ...  \n",
       "468466               0.0  \n",
       "468467               0.0  \n",
       "468468               0.0  \n",
       "468469               0.0  \n",
       "468470               0.0  \n",
       "\n",
       "[468471 rows x 33 columns]"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_std_point = pd.concat(max_std_point).reset_index()\n",
    "\n",
    "max_std_point.longitude = max_std_point.longitude.round(decimals=4)\n",
    "max_std_point.latitude = max_std_point.latitude.round(decimals=4)\n",
    "\n",
    "predict = predict.merge(max_std_point,on=['time','latitude','longitude'])\n",
    "\n",
    "predict=predict.dropna()\n",
    "predict=predict.loc[predict.unknown>0]\n",
    "predict=predict.groupby(['time','latitude','longitude']).max().reset_index(drop=True)\n",
    "predict = predict.rename(columns={'unknown':'max_std_point'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db0afab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b52067",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2dc86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# duration_point\n",
    "mrms_15\n",
    "\n",
    "values=[]\n",
    "for i in predict.index:\n",
    "    start = predict.time[i]\n",
    "    end = predict.time[i]+np.timedelta64(8, 'h')\n",
    "    s = mrms_15.sel(time=slice(start,end),latitude=predict.latitude[i],longitude=predict.longitude[i])\n",
    "    \n",
    "    values.append(s)\n",
    "    \n",
    "t = np.arange(0,len(values),1)\n",
    "sample = np.array_split(t,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23137474",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "values_t=[]\n",
    "for i in range(len(sample)):\n",
    "    print(i)\n",
    "    shape = len(sample[i])\n",
    "    st = xr.concat(values[sample[i][0]:sample[i][-1]],dim='time')\n",
    "    st = st.to_numpy()\n",
    "    values_t.append(np.reshape(st,(shape,241)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acae55e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "new = np.concatenate(new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090038b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_int_point\n",
    "mrms_one = m_g.resample('1min').asfreq()\n",
    "mrms_one.unknown = mrms_one.unknown.fillna(0)\n",
    "mrms_one.unknown = (mrms_one.unknown.rolling(15,min_periods=1).sum())*(60/15) \n",
    "\n",
    "\n",
    "# duration_point\n",
    "\n",
    "# std\n",
    "\n",
    "# var\n",
    "\n",
    "# mean\n",
    "\n",
    "# median\n",
    "\n",
    "\n",
    "# average accum point\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3164ee9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "storm_id = []\n",
    "\n",
    "for i in range(len(filenames_storm)):\n",
    "    storm = xr.open_dataset(storm_folder+'\\\\'+filenames_storm[i],chunks={'time': '50MB'})\n",
    "    storm['storm_id'] = storm.storm_id.astype('float64')\n",
    "    storm = storm+unique[i]\n",
    "    storm_id.append(storm)\n",
    "\n",
    "storm_id = xr.concat(storm_id,dim='time')\n",
    "storm_id = storm_id.sortby('time')\n",
    "storm_id = time_change(storm_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53213686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find storms during 8hr window\n",
    "storms=[]\n",
    "for i in predict.index:\n",
    "    start = predict.time[i]\n",
    "    end = predict.time[i]+np.timedelta64(8, 'h')\n",
    "    s = storm_id.sel(time=slice(start,end),latitude=predict.latitude[i],longitude=predict.longitude[i])\n",
    "    \n",
    "    # fill 8 hr window with nan where no storm\n",
    "    x=pd.date_range(start=start,end=end,periods=241)\n",
    "    d = {'time':x}\n",
    "    temp =pd.DataFrame(d)\n",
    "    temp['storm_id']=np.nan\n",
    "    temp =temp.set_index(['time'])\n",
    "    temp = temp.to_xarray()\n",
    "    s = xr.merge([s,temp],compat='override')\n",
    "    \n",
    "    storms.append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0fdb8004",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "CPU times: total: 5h 11min 7s\n",
      "Wall time: 6h 22min 55s\n"
     ]
    }
   ],
   "source": [
    "# get only unique storms id\n",
    "t = np.arange(0,len(storms),1)\n",
    "sample = np.array_split(t,100)\n",
    "\n",
    "storm_t=[]\n",
    "for i in range(len(sample)):\n",
    "    #print(i)\n",
    "    shape = len(sample[i])\n",
    "    st = xr.concat(storms[sample[i][0]:sample[i][-1]],dim='time')\n",
    "    st = st.storm_id.to_numpy()\n",
    "    storm_t.append(np.reshape(st,(shape,241)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "db4eb83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "new = []\n",
    "for i in range(len(storm_t)):\n",
    "    test = np.append(storm_t[i],storms[sample[i][-1]].storm_id.to_numpy())\n",
    "    shape=len(sample[i])\n",
    "    test=np.reshape(test,(shape,241))\n",
    "    new.append(test)\n",
    "    \n",
    "new = np.concatenate(new)\n",
    "\n",
    "storm_t = [np.unique(new[i]) for i in range(len(new))]\n",
    "\n",
    "storm_t = [storm_t[i][~np.isnan(storm_t[i])] for i in range(len(storm_t))]\n",
    "\n",
    "storm_t=[pd.DataFrame(storm_t[i]) for i in range(len(storm_t))]\n",
    "storm_t=[storm_t[i].loc[storm_t[i][0]>=1].values for i in range(len(storm_t))]\n",
    "\n",
    "predict['storm_id'] = storm_t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86044c4",
   "metadata": {},
   "source": [
    "get storm values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "4be5abd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open accum\n",
    "accum_max = []\n",
    "accum_mean = []\n",
    "for i in range(len(filenames_accum)):\n",
    "    s = pd.read_feather(precip_folder+'\\\\'+filenames_accum[i])\n",
    "    s.storm_id = s.storm_id+unique[i]\n",
    "    accum_max.append(s.groupby(['storm_id']).max())\n",
    "    accum_mean.append(s.groupby(['storm_id']).mean())\n",
    "    \n",
    "accum_max = pd.concat(accum_max).reset_index()\n",
    "accum_mean = pd.concat(accum_mean).reset_index()\n",
    "\n",
    "accum_mean = [accum_mean.loc[accum_mean.storm_id.isin(predict.storm_id[i])].mean() for i in range(len(predict))]\n",
    "accum_mean = pd.concat(accum_mean,axis=1).T.reset_index()\n",
    "predict['accum_mean_storm'] = accum_mean.unknown\n",
    "\n",
    "accum_max = [accum_max.loc[accum_max.storm_id.isin(predict.storm_id[i])].mean() for i in range(len(predict))]\n",
    "accum_max = pd.concat(accum_max,axis=1).T.reset_index()\n",
    "predict['accum_max_storm'] = accum_max.unknown\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "e0b80d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(precip_folder) \n",
    "filenames_stats = glob.glob('*_stats')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "6f411931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_storm_int_max\n",
    "\n",
    "# max_storm_int_std\n",
    "\n",
    "# max_storm_int_var\n",
    "\n",
    "# max_storm_int_mean\n",
    "\n",
    "# max_storm_int_median\n",
    "\n",
    "stats = []\n",
    "for i in range(len(filenames_stats)):\n",
    "    s = pd.read_feather(precip_folder+'\\\\'+filenames_stats[i])\n",
    "    s.storm_id = s.storm_id+unique[i]\n",
    "    stats.append(s)\n",
    "stats = pd.concat(stats)\n",
    "\n",
    "stats = [stats.loc[stats.storm_id.isin(predict.storm_id[i])].max() for i in range(len(predict))] ############not sure if this should be max or mean\n",
    "stats = pd.concat(stats,axis=1).T\n",
    "stats = stats.drop(columns=['storm_id'])\n",
    "stats = stats.reset_index()\n",
    "predict = pd.concat([predict,stats],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "4c6c5625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# elev_storm_centroid\n",
    "os.chdir(precip_folder) \n",
    "filenames_elev = glob.glob('*_elev1')\n",
    "\n",
    "\n",
    "elev = []\n",
    "for i in range(len(filenames_elev)):\n",
    "    s = pd.read_feather(precip_folder+'\\\\'+filenames_elev[i])\n",
    "    s.storm_id = s.storm_id+unique[i]\n",
    "    elev.append(s)\n",
    "elev = pd.concat(elev)\n",
    "elev = [elev.loc[elev.storm_id.isin(predict.storm_id[i])].mean() for i in range(len(predict))]\n",
    "elev = pd.concat(elev,axis=1).T.reset_index()\n",
    "predict['storm_elev'] = elev.elevation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "fe032e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# elevation_foot\n",
    "# slope_foot\n",
    "\n",
    "os.chdir(precip_folder) \n",
    "filenames_se = glob.glob('*_slope_elev')\n",
    "\n",
    "se=[]\n",
    "for i in range(len(filenames_se)):\n",
    "    s = pd.read_feather(precip_folder+'\\\\'+filenames_se[i])\n",
    "    s.storm_id = s.storm_id+unique[i]\n",
    "    se.append(s)\n",
    "slope_elev = pd.concat(se)\n",
    "slope_elev = slope_elev.rename(columns={\"elevation\": \"elevation_foot\", \"slope\": \"slope_foot\"})\n",
    "#slope_elev['slope_foot'] = np.where(slope_elev['slope_foot'] <0, 0, slope_elev['slope_foot'])\n",
    "\n",
    "slope_elev = [slope_elev.loc[slope_elev.storm_id.isin(predict.storm_id[i])].mean() for i in range(len(predict))] ############not sure if this should be max or mean\n",
    "slope_elev = pd.concat(slope_elev,axis=1).T\n",
    "slope_elev = slope_elev.drop(columns=['storm_id'])\n",
    "slope_elev = slope_elev.reset_index()\n",
    "\n",
    "predict = pd.concat([predict,slope_elev],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "8cb2a2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# storm_aspect\n",
    "os.chdir(precip_folder) \n",
    "filenames_asp = glob.glob('*_aspect')\n",
    "\n",
    "asp=[]\n",
    "for i in range(len(filenames_asp)):\n",
    "    s = pd.read_feather(precip_folder+'\\\\'+filenames_asp[i])\n",
    "    s.storm_id = s.storm_id+unique[i]\n",
    "    asp.append(s)\n",
    "aspect = pd.concat(asp)\n",
    "\n",
    "aspect = [aspect.loc[aspect.storm_id.isin(predict.storm_id[i])].mean() for i in range(len(predict))]\n",
    "aspect = pd.concat(aspect,axis=1).T.reset_index()\n",
    "predict['storm_aspect'] = aspect.aspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "3665b409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# duration_storm\n",
    "os.chdir(precip_folder) \n",
    "filenames_dur = glob.glob('*_dur')\n",
    "\n",
    "dur = []\n",
    "for i in range(len(filenames_dur)):\n",
    "    s = pd.read_feather(precip_folder+'\\\\'+filenames_dur[i])\n",
    "    s.storm_id = s.storm_id+unique[i]\n",
    "    dur.append(s)\n",
    "\n",
    "dur = pd.concat(dur)\n",
    "dur = [dur.loc[dur.storm_id.isin(predict.storm_id[i])].mean() for i in range(len(predict))]\n",
    "dur = pd.concat(dur,axis=1).T.reset_index()\n",
    "predict['duration_storm'] = dur['duration (min)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "8aba9534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# duration_storm\n",
    "os.chdir(precip_folder) \n",
    "filenames_shape = glob.glob('*_shape')\n",
    "\n",
    "\n",
    "shape = []\n",
    "for i in range(len(filenames_shape)):\n",
    "    s = pd.read_feather(precip_folder+'\\\\'+filenames_shape[i])\n",
    "    s.storm_id = s.storm_id+unique[i]\n",
    "    shape.append(s)\n",
    "shape = pd.concat(shape)\n",
    "shape = [shape.loc[shape.storm_id.isin(predict.storm_id[i])].mean() for i in range(len(predict))]\n",
    "shape = pd.concat(shape,axis=1).T.reset_index()\n",
    "shape = shape.drop(columns=['storm_id']).reset_index()\n",
    "predict = pd.concat([predict,shape],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "038b6e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# velocity_storm\n",
    "os.chdir(precip_folder) \n",
    "filenames_vel = glob.glob('*_vel')\n",
    "\n",
    "vel = []\n",
    "for i in range(len(filenames_vel)):\n",
    "    s = pd.read_feather(precip_folder+'\\\\'+filenames_vel[i])\n",
    "    s.storm_id = s.storm_id+unique[i]\n",
    "    vel.append(s)\n",
    "\n",
    "vel = pd.concat(vel)\n",
    "\n",
    "vel = [vel.loc[vel.storm_id.isin(predict.storm_id[i])].mean() for i in range(len(predict))]\n",
    "vel = pd.concat(vel,axis=1).T.reset_index()\n",
    "\n",
    "\n",
    "predict['velocity_storm']=vel.velocity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e92184",
   "metadata": {},
   "source": [
    "get point precip values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a18c8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_int_point\n",
    "\n",
    "# totalaccum_point\n",
    "\n",
    "# duration_point\n",
    "\n",
    "# std\n",
    "\n",
    "# var\n",
    "\n",
    "# mean\n",
    "\n",
    "# median\n",
    "\n",
    "\n",
    "# average accum point\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af67fbe",
   "metadata": {},
   "source": [
    "get other point values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab0e4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# latitude\n",
    "\n",
    "# longitude\n",
    "\n",
    "# month\n",
    "\n",
    "# hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26c9163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mult_correct\n",
    "# get one value at a time?\n",
    "# convert to pandas and sort??\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4c9689",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RQI\n",
    "# bring in RQI\n",
    "# open RQI for 2021,2022\n",
    "RQI_folder = os.path.join(parentDir,\"MRMS\",\"RQI_cat_yr_CO\")\n",
    "filenames_RQI = glob.glob(RQI_folder+'\\\\'+'*.grib2')\n",
    "\n",
    "RQI = xr.open_mfdataset(filenames_RQI,engine = \"cfgrib\",chunks={'time': '1MB'})\n",
    "\n",
    "# get times and gage locations\n",
    "RQI = RQI.sel(longitude=lon,latitude=lat,drop=True)\n",
    "\n",
    "# change to MST\n",
    "RQI = time_change(RQI)\n",
    "\n",
    "r = []\n",
    "for i in range(len(compare)):\n",
    "    # select mrms at coordinate\n",
    "    r_g = RQI.sel(longitude=compare.longitude[i],latitude=compare.latitude[i],drop=True)\n",
    "    \n",
    "    r.append(r_g.sel(time=slice(compare.start[i].round('H'),compare.end[i].round('H'))).unknown.mean().compute().values)\n",
    "\n",
    "# add to database\n",
    "compare['RQI']=r\n",
    "compare.RQI.loc[compare['RQI']<0] = np.nan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b03fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# point_elev\n",
    "elev = '\\\\CO_SRTM1arcsec__merge.tif'\n",
    "folder = parentDir\n",
    "elev = folder+elev\n",
    "codtm = rxr.open_rasterio(elev)\n",
    "# change lon to match global lat/lon in grib file\n",
    "codtm = codtm.assign_coords(x=(((codtm.x + 360))))\n",
    "\n",
    "codtm = codtm.rename({'x':'longitude','y':'latitude'})\n",
    "\n",
    "elevation = [codtm.sel(longitude=compare.longitude[i],latitude=compare.latitude[i],\n",
    "                       method='nearest').values[0] for i in range(len(compare))]\n",
    "#elevation = codtm.sel(longitude=lon,latitude=lat,method='nearest')\n",
    "\n",
    "s = '\\\\CO_SRTM1arcsec_slope.tif'\n",
    "s = folder+s\n",
    "coslope = rxr.open_rasterio(s)\n",
    "# change lon to match global lat/lon in grib file\n",
    "coslope = coslope.assign_coords(x=(((coslope.x + 360))))\n",
    "\n",
    "coslope = coslope.rename({'x':'longitude','y':'latitude'})\n",
    "\n",
    "#slope = coslope.sel(longitude=lon,latitude=lat,method='nearest')\n",
    "slope = [coslope.sel(longitude=compare.longitude[i],latitude=compare.latitude[i],\n",
    "                     method='nearest').values[0] for i in range(len(compare))]\n",
    "\n",
    "compare['point_elev']=elevation\n",
    "compare['point_slope']=slope\n",
    "\n",
    "compare.point_slope=compare.point_slope.where(compare.point_slope.between(0,100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5fc7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# point_aspect\n",
    "# aspect at gage\n",
    "asp = '\\\\CO_SRTM1arcsec_aspect.tif'\n",
    "folder = parentDir\n",
    "asp = folder+asp\n",
    "codtm = rxr.open_rasterio(asp)\n",
    "# change lon to match global lat/lon in grib file\n",
    "codtm = codtm.assign_coords(x=(((codtm.x + 360))))\n",
    "\n",
    "codtm = codtm.rename({'x':'longitude','y':'latitude'})\n",
    "\n",
    "aspect = [codtm.sel(longitude=compare.longitude[i],latitude=compare.latitude[i],\n",
    "                       method='nearest').values[0] for i in range(len(compare))]\n",
    "compare['point_aspect']=aspect"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
