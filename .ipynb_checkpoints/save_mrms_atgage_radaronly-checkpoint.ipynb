{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42a5c9c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Z:\\working code\\mrms_eval\\grizzly.py:23: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  gage = [gage[i].drop(gage[i].columns.difference(['TimeStamp (Local)','15-minute Intensity (mm/h)','Bin Accum (mm)']), 1) for i in range(len(gage_id))]\n",
      "Z:\\working code\\mrms_eval\\usgs_other.py:9: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  gage = [gage[i].drop(gage[i].columns.difference(['time','precip_mm_diff','intensity_15m']), 1) for i in range(len(gage))]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import xarray as xr\n",
    "from dask.distributed import Client\n",
    "import matplotlib.pyplot as plt\n",
    "from os import listdir\n",
    "import rioxarray as rxr\n",
    "import glob\n",
    "\n",
    "from CPF import *\n",
    "from grizzly import *\n",
    "from coagmet import *\n",
    "from disdrometer import *\n",
    "from mrms_gage import *\n",
    "from usgs_other import *\n",
    "from elevation import *\n",
    "from usgs_new import *\n",
    "\n",
    "\n",
    "gage_folder = os.path.join(parentDir,\"precip_gage\")\n",
    "\n",
    "# grizzly\n",
    "grizzly = get_grizzly(gage_folder)\n",
    "# cpf\n",
    "cpf = get_cpf(gage_folder+'\\\\cpf google drive')\n",
    "# disdrometer\n",
    "disdrom = get_disdrom(gage_folder)\n",
    "# coagmet\n",
    "coag = get_coagmet(gage_folder)\n",
    "# other gages\n",
    "other = get_usgs_other(gage_folder)\n",
    "\n",
    "new_usgs = get_usgsnew(gage_folder)\n",
    "\n",
    "# bring everything together\n",
    "gage = {**grizzly, **cpf, **disdrom, **coag, **other, **new_usgs}\n",
    "\n",
    "# get list of keys (lat/lon)\n",
    "coord = [i for i in gage.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad835e56",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lat = [coord[i][0] for i in range(len(coord))]\n",
    "lon = [coord[i][1] for i in range(len(coord))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "acc6e04f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring index file 'Z:\\\\MRMS\\\\2min_rate_cat_month_CO\\\\eval\\\\may_2021_rate_CO.grib2.923a8.idx' incompatible with GRIB file\n",
      "Ignoring index file 'Z:\\\\MRMS\\\\2min_rate_cat_month_CO\\\\eval\\\\may_2022_rate_CO.grib2.923a8.idx' incompatible with GRIB file\n",
      "Ignoring index file 'Z:\\\\MRMS\\\\2min_rate_cat_month_CO\\\\eval\\\\jun_2021_rate_CO.grib2.923a8.idx' incompatible with GRIB file\n",
      "Ignoring index file 'Z:\\\\MRMS\\\\2min_rate_cat_month_CO\\\\eval\\\\jun_2022_rate_CO.grib2.923a8.idx' incompatible with GRIB file\n",
      "Ignoring index file 'Z:\\\\MRMS\\\\2min_rate_cat_month_CO\\\\eval\\\\jul_2021_rate_CO.grib2.923a8.idx' incompatible with GRIB file\n",
      "Ignoring index file 'Z:\\\\MRMS\\\\2min_rate_cat_month_CO\\\\eval\\\\jul_2022_rate_CO.grib2.923a8.idx' incompatible with GRIB file\n",
      "Can't read index file 'Z:\\\\MRMS\\\\2min_rate_cat_month_CO\\\\eval\\\\aug_2021_rate_CO.grib2.923a8.idx'\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\whitep\\Miniconda3\\envs\\radar\\lib\\site-packages\\cfgrib\\messages.py\", line 547, in from_indexpath_or_filestream\n",
      "    self = cls.from_indexpath(indexpath)\n",
      "  File \"C:\\Users\\whitep\\Miniconda3\\envs\\radar\\lib\\site-packages\\cfgrib\\messages.py\", line 429, in from_indexpath\n",
      "    index = pickle.load(file)\n",
      "EOFError: Ran out of input\n",
      "Ignoring index file 'Z:\\\\MRMS\\\\2min_rate_cat_month_CO\\\\eval\\\\aug_2022_rate_CO.grib2.923a8.idx' incompatible with GRIB file\n",
      "Ignoring index file 'Z:\\\\MRMS\\\\2min_rate_cat_month_CO\\\\eval\\\\sep_2021_rate_CO.grib2.923a8.idx' incompatible with GRIB file\n",
      "Ignoring index file 'Z:\\\\MRMS\\\\2min_rate_cat_month_CO\\\\eval\\\\sep_2022_rate_CO.grib2.923a8.idx' incompatible with GRIB file\n",
      "C:\\Users\\whitep\\Miniconda3\\envs\\radar\\lib\\site-packages\\xarray\\core\\indexing.py:1379: PerformanceWarning: Slicing is producing a large chunk. To accept the large\n",
      "chunk and silence this warning, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': False}):\n",
      "    ...     array[indexer]\n",
      "\n",
      "To avoid creating the large chunks, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': True}):\n",
      "    ...     array[indexer]\n",
      "  return self.array[key]\n",
      "C:\\Users\\whitep\\Miniconda3\\envs\\radar\\lib\\site-packages\\xarray\\core\\indexing.py:1385: PerformanceWarning: Slicing is producing a large chunk. To accept the large\n",
      "chunk and silence this warning, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': False}):\n",
      "    ...     array[indexer]\n",
      "\n",
      "To avoid creating the large chunks, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': True}):\n",
      "    ...     array[indexer]\n",
      "  value = value[(slice(None),) * axis + (subkey,)]\n",
      "C:\\Users\\whitep\\Miniconda3\\envs\\radar\\lib\\site-packages\\xarray\\core\\indexing.py:1379: PerformanceWarning: Slicing is producing a large chunk. To accept the large\n",
      "chunk and silence this warning, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': False}):\n",
      "    ...     array[indexer]\n",
      "\n",
      "To avoid creating the large chunks, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': True}):\n",
      "    ...     array[indexer]\n",
      "  return self.array[key]\n",
      "C:\\Users\\whitep\\Miniconda3\\envs\\radar\\lib\\site-packages\\xarray\\core\\indexing.py:1385: PerformanceWarning: Slicing is producing a large chunk. To accept the large\n",
      "chunk and silence this warning, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': False}):\n",
      "    ...     array[indexer]\n",
      "\n",
      "To avoid creating the large chunks, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': True}):\n",
      "    ...     array[indexer]\n",
      "  value = value[(slice(None),) * axis + (subkey,)]\n"
     ]
    }
   ],
   "source": [
    "mrms_rate_folder = os.path.join(parentDir,\"MRMS\",\"2min_rate_cat_month_CO\",\"eval\")\n",
    "months = ['may','jun','jul','aug','sep']\n",
    "years = [2021,2022]\n",
    "\n",
    "filenames_rate = [mrms_rate_folder+'\\\\'+i+'_'+str(j)+'_rate_CO.grib2' for i in months for j in years]\n",
    "\n",
    "mrms_rate = xr.open_mfdataset(filenames_rate,engine = \"cfgrib\",chunks={'time': '500MB'})\n",
    "mrms_rate = mrms_rate.where(mrms_rate.longitude<=256,drop=True)\n",
    "\n",
    "mrms_rate = mrms_rate.sel(longitude=lon,latitude=lat,method='nearest',drop=True)\n",
    "mrms_rate = mrms_rate.drop_duplicates(dim=['latitude','longitude'])\n",
    "\n",
    "# open storm_ids\n",
    "storm_folder = os.path.join(parentDir,\"CO_storm_output\")\n",
    "\n",
    "filenames_storm = [i+'_'+str(j)+'storm_id.nc' for i in months for j in years]\n",
    "\n",
    "storm_id = []\n",
    "# add float to make storm_ids unique for each month \n",
    "unique = np.arange(0,0.1,0.01)\n",
    "\n",
    "for i in range(len(filenames_storm)):\n",
    "    storm = xr.open_dataset(storm_folder+'\\\\'+filenames_storm[i],chunks={'time': '500MB'})\n",
    "    storm['storm_id'] = storm.storm_id.astype('float64')\n",
    "    storm = storm+unique[i]\n",
    "    storm_id.append(storm)\n",
    "\n",
    "storm_id = xr.concat(storm_id,dim='time')\n",
    "storm_id = storm_id.sortby('time')\n",
    "\n",
    "storm_id = storm_id.sel(longitude=lon,latitude=lat,method='nearest',drop=True)\n",
    "storm_id = storm_id.drop_duplicates(dim=['latitude','longitude'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "078eadec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring index file 'Z:\\\\MRMS\\\\1hr_QPE_multi_cat_yr_CO\\\\2022_multiQPE_CO.grib2.923a8.idx' incompatible with GRIB file\n",
      "C:\\Users\\whitep\\Miniconda3\\envs\\radar\\lib\\site-packages\\xarray\\core\\indexing.py:1379: PerformanceWarning: Slicing is producing a large chunk. To accept the large\n",
      "chunk and silence this warning, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': False}):\n",
      "    ...     array[indexer]\n",
      "\n",
      "To avoid creating the large chunks, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': True}):\n",
      "    ...     array[indexer]\n",
      "  return self.array[key]\n",
      "C:\\Users\\whitep\\Miniconda3\\envs\\radar\\lib\\site-packages\\xarray\\core\\indexing.py:1385: PerformanceWarning: Slicing is producing a large chunk. To accept the large\n",
      "chunk and silence this warning, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': False}):\n",
      "    ...     array[indexer]\n",
      "\n",
      "To avoid creating the large chunks, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': True}):\n",
      "    ...     array[indexer]\n",
      "  value = value[(slice(None),) * axis + (subkey,)]\n",
      "C:\\Users\\whitep\\Miniconda3\\envs\\radar\\lib\\site-packages\\xarray\\core\\indexing.py:1379: PerformanceWarning: Slicing is producing a large chunk. To accept the large\n",
      "chunk and silence this warning, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': False}):\n",
      "    ...     array[indexer]\n",
      "\n",
      "To avoid creating the large chunks, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': True}):\n",
      "    ...     array[indexer]\n",
      "  return self.array[key]\n",
      "C:\\Users\\whitep\\Miniconda3\\envs\\radar\\lib\\site-packages\\xarray\\core\\indexing.py:1385: PerformanceWarning: Slicing is producing a large chunk. To accept the large\n",
      "chunk and silence this warning, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': False}):\n",
      "    ...     array[indexer]\n",
      "\n",
      "To avoid creating the large chunks, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': True}):\n",
      "    ...     array[indexer]\n",
      "  value = value[(slice(None),) * axis + (subkey,)]\n",
      "C:\\Users\\whitep\\Miniconda3\\envs\\radar\\lib\\site-packages\\xarray\\core\\indexing.py:1379: PerformanceWarning: Slicing is producing a large chunk. To accept the large\n",
      "chunk and silence this warning, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': False}):\n",
      "    ...     array[indexer]\n",
      "\n",
      "To avoid creating the large chunks, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': True}):\n",
      "    ...     array[indexer]\n",
      "  return self.array[key]\n"
     ]
    }
   ],
   "source": [
    "# multisensor correction\n",
    "# open multisensor qpe for 2021,2022\n",
    "mrms_multi_folder = os.path.join(parentDir,\"MRMS\",\"1hr_QPE_multi_cat_yr_CO\")\n",
    "filenames_multi = glob.glob(mrms_multi_folder+'\\\\'+'*.grib2')\n",
    "mrms_multi = xr.open_mfdataset(filenames_multi,engine = \"cfgrib\",chunks={'time': '500MB'})\n",
    "mrms_multi = mrms_multi.where(mrms_multi>=0)\n",
    "mrms_multi = mrms_multi.where(mrms_multi.longitude<=256,drop=True)\n",
    "\n",
    "mrms_multi = mrms_multi.sel(longitude=lon,latitude=lat,method='nearest',drop=True)\n",
    "mrms_multi = mrms_multi.drop_duplicates(dim=['latitude','longitude'])\n",
    "\n",
    "# open radaronly qpe for 2021,2022\n",
    "mrms_radar_folder = os.path.join(parentDir,\"MRMS\",\"1hr_QPE_radar_cat_yr_CO\")\n",
    "filenames_radar = glob.glob(mrms_radar_folder+'\\\\'+'*.grib2')\n",
    "mrms_radar = xr.open_mfdataset(filenames_radar,engine = \"cfgrib\",chunks={'time': '500MB'})\n",
    "mrms_radar = mrms_radar.where(mrms_radar>=0)\n",
    "mrms_radar = mrms_radar.where(mrms_radar.longitude<=256,drop=True)\n",
    "\n",
    "mrms_radar = mrms_radar.sel(longitude=lon,latitude=lat,method='nearest',drop=True)\n",
    "mrms_radar = mrms_radar.drop_duplicates(dim=['latitude','longitude'])\n",
    "\n",
    "correction = (mrms_multi/mrms_radar)\n",
    "correction = correction.where(correction.unknown != np.inf).fillna(1)\n",
    "correction = correction.resample(time='2min').pad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f205697e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mrms_accum = mrms_rate*(2/60)\n",
    "mrms_2_corrected = mrms_accum*correction\n",
    "\n",
    "# change time to MST\n",
    "mrms_2_corrected = time_change(mrms_2_corrected)\n",
    "storm_id = time_change(storm_id)\n",
    "\n",
    "m = mrms_2_corrected\n",
    "s = storm_id\n",
    "\n",
    "m = m.assign({\"storm_id\": s.storm_id})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "61b88dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = m.chunk({'time': '10MB'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c4d754f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\whitep\\Miniconda3\\envs\\radar\\lib\\site-packages\\xarray\\core\\indexing.py:1379: PerformanceWarning: Slicing is producing a large chunk. To accept the large\n",
      "chunk and silence this warning, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': False}):\n",
      "    ...     array[indexer]\n",
      "\n",
      "To avoid creating the large chunks, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': True}):\n",
      "    ...     array[indexer]\n",
      "  return self.array[key]\n",
      "C:\\Users\\whitep\\Miniconda3\\envs\\radar\\lib\\site-packages\\xarray\\core\\indexing.py:1379: PerformanceWarning: Slicing is producing a large chunk. To accept the large\n",
      "chunk and silence this warning, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': False}):\n",
      "    ...     array[indexer]\n",
      "\n",
      "To avoid creating the large chunks, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': True}):\n",
      "    ...     array[indexer]\n",
      "  return self.array[key]\n",
      "C:\\Users\\whitep\\Miniconda3\\envs\\radar\\lib\\site-packages\\xarray\\core\\indexing.py:1379: PerformanceWarning: Slicing is producing a large chunk. To accept the large\n",
      "chunk and silence this warning, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': False}):\n",
      "    ...     array[indexer]\n",
      "\n",
      "To avoid creating the large chunks, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': True}):\n",
      "    ...     array[indexer]\n",
      "  return self.array[key]\n",
      "C:\\Users\\whitep\\Miniconda3\\envs\\radar\\lib\\site-packages\\xarray\\core\\indexing.py:1379: PerformanceWarning: Slicing is producing a large chunk. To accept the large\n",
      "chunk and silence this warning, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': False}):\n",
      "    ...     array[indexer]\n",
      "\n",
      "To avoid creating the large chunks, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': True}):\n",
      "    ...     array[indexer]\n",
      "  return self.array[key]\n"
     ]
    }
   ],
   "source": [
    "m = m.sortby(m.latitude)\n",
    "m = m.sortby(m.longitude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24caae3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = m.where(m.storm_id>=1,drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5a827b78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\whitep\\Miniconda3\\envs\\radar\\lib\\site-packages\\dask\\core.py:119: RuntimeWarning: divide by zero encountered in divide\n",
      "  return func(*(_execute_task(a, cache) for a in args))\n"
     ]
    }
   ],
   "source": [
    "name = 'test.nc'\n",
    "path = parentDir+name\n",
    "m.to_netcdf(path=path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "d0aecf8d292e5c15734fc4afa3256eebe782d67300dfb49589358bb0d38ef1e5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
