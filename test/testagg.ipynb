{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f3a0ec7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring index file 'Z:\\\\PhoebeRadar\\\\MRMS\\\\2min_rate_cat_month_CO\\\\eval\\\\aug_2022_rate_CO.grib2.923a8.idx' incompatible with GRIB file\n",
      "C:\\Users\\whitep\\Miniconda3\\envs\\radar\\lib\\site-packages\\xarray\\core\\indexing.py:1374: PerformanceWarning: Slicing is producing a large chunk. To accept the large\n",
      "chunk and silence this warning, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': False}):\n",
      "    ...     array[indexer]\n",
      "\n",
      "To avoid creating the large chunks, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': True}):\n",
      "    ...     array[indexer]\n",
      "  return self.array[key]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1min 54s\n",
      "Wall time: 4min 23s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import skimage.measure\n",
    "import pandas as pd\n",
    "from scipy.spatial import distance\n",
    "import skimage.measure\n",
    "from skimage.morphology import remove_small_objects,closing,binary_closing\n",
    "from scipy import ndimage\n",
    "from datetime import datetime,timedelta\n",
    "import os\n",
    "from dask.distributed import Client\n",
    "import glob\n",
    "from shapely.geometry import MultiPoint\n",
    "import geopandas as gpd\n",
    "import cartopy.crs as ccrs\n",
    "\n",
    "client = Client()\n",
    "\n",
    "# Create a path to the code file\n",
    "codeDir = os.path.dirname(os.path.abspath(os.getcwd()))\n",
    "parentDir = os.path.dirname(codeDir)\n",
    "\n",
    "mrms_folder = os.path.join(parentDir,\"MRMS\",\"2min_rate_cat_month_CO\",\"eval\")\n",
    "storm_folder = os.path.join(parentDir,\"CO_storm_output\")\n",
    "\n",
    "os.chdir(mrms_folder)\n",
    "filenames_rate = glob.glob('*.grib2')\n",
    "os.chdir(storm_folder)\n",
    "filenames_storm = glob.glob('*.nc')\n",
    "\n",
    "\n",
    "month = xr.open_dataset(mrms_folder+'\\\\'+filenames_rate[1], engine = \"cfgrib\",chunks={'time': '500MB'})\n",
    "month = month.where(month.longitude<=256,drop=True)\n",
    "\n",
    "storm = xr.open_dataset(storm_folder+'\\\\'+filenames_storm[1],chunks={'time': '500MB'})\n",
    "start=month.time[0].values\n",
    "end=start+np.timedelta64(1,'D')\n",
    "# create test slice of time\n",
    "month = month.sel(time=slice(start,end))\n",
    "storm = storm.sel(time=slice(start,end))\n",
    "\n",
    "# get storm ids\n",
    "storms = storm.storm_id.values\n",
    "storms = storms[~np.isnan(storms)]\n",
    "storms = storms[storms>0]\n",
    "storms = np.unique(storms)\n",
    "\n",
    "# split into sections\n",
    "sample = np.array_split(storms,20)\n",
    "\n",
    "velocity = []\n",
    "duration = []\n",
    "storm_accum = []\n",
    "storm_id=[]\n",
    "\n",
    "for i in range(1):\n",
    "    sample_storms = storm.where(storm.storm_id.isin(sample[i]),drop=True)\n",
    "    sample_month = month.sel(time=sample_storms.time,latitude = sample_storms.latitude, \n",
    "                             longitude = sample_storms.longitude,method='nearest')\n",
    "    sample_month = sample_month.assign({\"storm_id\": sample_storms.storm_id})\n",
    "\n",
    "    sample_month = sample_month.to_dataframe()\n",
    "    sample_month['unknown'] = sample_month.unknown*(2/60)\n",
    "\n",
    "    for s in sample[0]:\n",
    "        accum = sample_month.loc[sample_month.storm_id==s].drop(['step','heightAboveSea','valid_time'],\n",
    "                                                        axis=1).reset_index().groupby(['latitude','longitude']).sum().unknown\n",
    "        accum['storm_id']=s\n",
    "        storm_accum.append(accum)\n",
    "\n",
    "        agg = sample_month.loc[sample_month.storm_id==s].drop(['step','heightAboveSea','valid_time'],\n",
    "                                                            axis=1).reset_index().groupby(['time']).agg(list)\n",
    "        centroid = []\n",
    "        for j in range(len(agg)):\n",
    "            latitude = agg.latitude[j]\n",
    "            longitude = agg.longitude[j]\n",
    "            gage_coord = np.stack((longitude,latitude),axis=1)\n",
    "            points = MultiPoint(gage_coord)\n",
    "            centroid.append(points.centroid) \n",
    "\n",
    "        # add nan to start of list to comare distance from last point\n",
    "        if len(centroid)<=1:\n",
    "            distance=np.nan\n",
    "            duration.append([s,2])\n",
    "        else:\n",
    "            fill = [np.nan]\n",
    "            centroid = fill+centroid\n",
    "            centroid_last = centroid[0:-1]\n",
    "            centroid = centroid[1:len(centroid)]\n",
    "            # reproject to get accurate distance\n",
    "            centroid = gpd.GeoSeries(centroid, crs='EPSG:4326')\n",
    "            centroid_last = gpd.GeoSeries(centroid_last, crs='EPSG:4326')\n",
    "            crs = ccrs.LambertConformal(central_latitude=38.5, central_longitude=-105)\n",
    "            centroid = centroid.to_crs(crs)\n",
    "            centroid_last = centroid.shift()\n",
    "            distance = centroid.distance(centroid_last)\n",
    "\n",
    "            storm_id.append(s)\n",
    "            velocity.append((distance/(2*60)).mean())\n",
    "\n",
    "            dur_timestamp = agg.index[-1]-agg.index[0]\n",
    "            dur_sec = dur_timestamp.seconds\n",
    "            dur_min = (dur_sec % 3600) // 60\n",
    "            duration.append(dur_min)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "829895aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring index file 'Z:\\\\PhoebeRadar\\\\MRMS\\\\2min_rate_cat_month_CO\\\\eval\\\\aug_2022_rate_CO.grib2.923a8.idx' incompatible with GRIB file\n",
      "C:\\Users\\whitep\\Miniconda3\\envs\\radar\\lib\\site-packages\\xarray\\core\\indexing.py:1374: PerformanceWarning: Slicing is producing a large chunk. To accept the large\n",
      "chunk and silence this warning, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': False}):\n",
      "    ...     array[indexer]\n",
      "\n",
      "To avoid creating the large chunks, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': True}):\n",
      "    ...     array[indexer]\n",
      "  return self.array[key]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1min 13s\n",
      "Wall time: 1min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import skimage.measure\n",
    "import pandas as pd\n",
    "from scipy.spatial import distance\n",
    "import skimage.measure\n",
    "from skimage.morphology import remove_small_objects,closing,binary_closing\n",
    "from scipy import ndimage\n",
    "from datetime import datetime,timedelta\n",
    "import os\n",
    "from dask.distributed import Client\n",
    "import glob\n",
    "from shapely.geometry import MultiPoint\n",
    "import geopandas as gpd\n",
    "import cartopy.crs as ccrs\n",
    "\n",
    "client = Client()\n",
    "\n",
    "# Create a path to the code file\n",
    "codeDir = os.path.dirname(os.path.abspath(os.getcwd()))\n",
    "parentDir = os.path.dirname(codeDir)\n",
    "\n",
    "mrms_folder = os.path.join(parentDir,\"MRMS\",\"2min_rate_cat_month_CO\",\"eval\")\n",
    "storm_folder = os.path.join(parentDir,\"CO_storm_output\")\n",
    "\n",
    "os.chdir(mrms_folder)\n",
    "filenames_rate = glob.glob('*.grib2')\n",
    "os.chdir(storm_folder)\n",
    "filenames_storm = glob.glob('*.nc')\n",
    "\n",
    "\n",
    "month = xr.open_dataset(mrms_folder+'\\\\'+filenames_rate[1], engine = \"cfgrib\",chunks={'time': '10MB'})\n",
    "month = month.where(month.longitude<=256,drop=True)\n",
    "\n",
    "storm = xr.open_dataset(storm_folder+'\\\\'+filenames_storm[1],chunks={'time': '10MB'})\n",
    "start=month.time[0].values\n",
    "end=start+np.timedelta64(1,'D')\n",
    "# create test slice of time\n",
    "month = month.sel(time=slice(start,end))\n",
    "storm = storm.sel(time=slice(start,end))\n",
    "\n",
    "# get storm ids\n",
    "storms = storm.storm_id.values\n",
    "storms = storms[~np.isnan(storms)]\n",
    "storms = storms[storms>0]\n",
    "storms = np.unique(storms)\n",
    "\n",
    "# split into sections\n",
    "sample = np.array_split(storms,20)\n",
    "\n",
    "velocity = []\n",
    "duration = []\n",
    "storm_accum = []\n",
    "storm_id=[]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309ac337",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "for i in range(1):\n",
    "    sample_storms = storm.where(storm.storm_id.isin(sample[i]),drop=True)\n",
    "    sample_month = month.sel(time=sample_storms.time,latitude = sample_storms.latitude, \n",
    "                             longitude = sample_storms.longitude,method='nearest')\n",
    "    sample_month = sample_month.assign({\"storm_id\": sample_storms.storm_id})\n",
    "\n",
    "    sample_month = sample_month.to_dataframe()\n",
    "    sample_month['unknown'] = sample_month.unknown*(2/60)\n",
    "\n",
    "    for s in sample[0]:\n",
    "        accum = sample_month.loc[sample_month.storm_id==s].drop(['step','heightAboveSea','valid_time'],\n",
    "                                                        axis=1).reset_index().groupby(['latitude','longitude']).sum().unknown\n",
    "        accum['storm_id']=s\n",
    "        storm_accum.append(accum)\n",
    "\n",
    "        agg = sample_month.loc[sample_month.storm_id==s].drop(['step','heightAboveSea'],\n",
    "                                                            axis=1).reset_index().groupby(['time']).agg(list)\n",
    "        agg = agg.resample('30min').agg(sum)\n",
    "        \n",
    "        centroid = []\n",
    "        for j in range(len(agg)):\n",
    "            latitude = agg.latitude[j]\n",
    "            longitude = agg.longitude[j]\n",
    "            gage_coord = np.stack((longitude,latitude),axis=1)\n",
    "            points = MultiPoint(gage_coord)\n",
    "            centroid.append(points.centroid) \n",
    "\n",
    "        # add nan to start of list to comare distance from last point\n",
    "        if len(centroid)<=1:\n",
    "            distance=np.nan\n",
    "            #duration.append([s,2])\n",
    "        else:\n",
    "            fill = [np.nan]\n",
    "            centroid = fill+centroid\n",
    "            centroid_last = centroid[0:-1]\n",
    "            centroid = centroid[1:len(centroid)]\n",
    "            # reproject to get accurate distance\n",
    "            centroid = gpd.GeoSeries(centroid, crs='EPSG:4326')\n",
    "            centroid_last = gpd.GeoSeries(centroid_last, crs='EPSG:4326')\n",
    "            crs = ccrs.LambertConformal(central_latitude=38.5, central_longitude=-105)\n",
    "            centroid = centroid.to_crs(crs)\n",
    "            centroid_last = centroid.shift()\n",
    "            distance = centroid.distance(centroid_last)\n",
    "\n",
    "            storm_id.append(s)\n",
    "            velocity.append((distance/(30*60)).mean())\n",
    "\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "358a6431",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list([40.57499999999929, 40.58499999999929, 40.57499999999929, 40.58499999999929])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agg.latitude.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "732c6d54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2022-08-01</th>\n",
       "      <td>[40.57499999999929, 40.58499999999929, 40.5749...</td>\n",
       "      <td>[253.4749989999977, 253.4749989999977, 253.474...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     latitude  \\\n",
       "time                                                            \n",
       "2022-08-01  [40.57499999999929, 40.58499999999929, 40.5749...   \n",
       "\n",
       "                                                    longitude  \n",
       "time                                                           \n",
       "2022-08-01  [253.4749989999977, 253.4749989999977, 253.474...  "
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "b9108113",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3599815091146419"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(velocity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4aa3c25e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9833727393536313"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(velocity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "7c2cc5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1):\n",
    "    sample_storms = storm.where(storm.storm_id.isin(sample[i]),drop=True)\n",
    "    sample_month = month.sel(time=sample_storms.time,latitude = sample_storms.latitude, \n",
    "                             longitude = sample_storms.longitude,method='nearest')\n",
    "    sample_month = sample_month.assign({\"storm_id\": sample_storms.storm_id})\n",
    "\n",
    "    sample_month = sample_month.to_dataframe()\n",
    "    sample_month['unknown'] = sample_month.unknown*(2/60)\n",
    "\n",
    "    for s in sample[0]:\n",
    "        accum = sample_month.loc[sample_month.storm_id==s].drop(['step','heightAboveSea','valid_time'],\n",
    "                                                        axis=1).reset_index().groupby(['latitude','longitude']).sum().unknown\n",
    "        accum['storm_id']=s\n",
    "        storm_accum.append(accum)\n",
    "\n",
    "        agg = sample_month.loc[sample_month.storm_id==s].drop(['step','heightAboveSea','valid_time'],\n",
    "                                                            axis=1).reset_index().groupby(['time']).agg(list)\n",
    "        centroid = []\n",
    "        for j in range(len(agg)):\n",
    "            latitude = agg.latitude[j]\n",
    "            longitude = agg.longitude[j]\n",
    "            gage_coord = np.stack((longitude,latitude),axis=1)\n",
    "            points = MultiPoint(gage_coord)\n",
    "            centroid.append(points.centroid) \n",
    "\n",
    "        # add nan to start of list to comare distance from last point\n",
    "        if len(centroid)<=1:\n",
    "            distance=np.nan\n",
    "            duration.append([s,2])\n",
    "        else:\n",
    "            fill = [np.nan]\n",
    "            centroid = fill+centroid\n",
    "            centroid_last = centroid[0:-1]\n",
    "            centroid = centroid[1:len(centroid)]\n",
    "            # reproject to get accurate distance\n",
    "            centroid = gpd.GeoSeries(centroid, crs='EPSG:4326')\n",
    "            centroid_last = gpd.GeoSeries(centroid_last, crs='EPSG:4326')\n",
    "            crs = ccrs.LambertConformal(central_latitude=38.5, central_longitude=-105)\n",
    "            centroid = centroid.to_crs(crs)\n",
    "            centroid_last = centroid.shift()\n",
    "            distance = centroid.distance(centroid_last)\n",
    "\n",
    "            storm_id.append(s)\n",
    "            velocity.append((distance/(2*60)).mean())\n",
    "\n",
    "            dur_timestamp = agg.index[-1]-agg.index[0]\n",
    "            dur_sec = dur_timestamp.seconds\n",
    "            dur_min = (dur_sec % 3600) // 60\n",
    "            duration.append(dur_min)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
